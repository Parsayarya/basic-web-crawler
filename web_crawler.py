# -*- coding: utf-8 -*-
"""web_crawler.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HZyaVm-ryqc8UTJBsNR3x15UEwkvB8Bv
"""

#import libraries we need
import requests # for send and reieve requests
import lxml
from xlwt import *
from bs4 import BeautifulSoup

# define an excel work book for write and show results
enviroment = Workbook(encoding = 'utf-8') # define format utf-8 for our workbook
# add one new sheet for excel file 
crawler_table = enviroment.add_sheet('data')
# we want to have 4 columns of data about our crawled data from web about movies!
crawler_table.write(0, 0, 'Number') # column for number 
crawler_table.write(0, 1, 'Name of Movie') # column for URL of movie
crawler_table.write(0, 2, 'URL of Movie') # column for name of movie
crawler_table.write(0, 3, 'Introduction of Movie') # column for descriptions of movie

#bring site we want to crawl 
site_url = "https://www.rottentomatoes.com/top/bestofrt/" # for movies
site_header = {
  'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36 QIHU 360SE' #browser Header
}
#get request for said url
got_requests = requests.get(site_url , headers = site_header)
list_of_movies = []
souped_requests = BeautifulSoup(got_requests.content, 'lxml')
all_movies = souped_requests.find('table', {'class': 'table'}).find_all('a')

#define two variables 
# selected_line for write data of four columns for each movie and fill table in excel
#number_of_movie for number column 
selected_line = 1
number_of_movie = 0

# initialy we should assign number of movies 0 because of better result print
number_of_movie = 0
# this loop for each movie print result of 4 columns
# then write in excel file with name crawler_Result.xls
for each_movie in all_movies:
  selected_url = 'https://www.rottentomatoes.com' + each_movie['href']
  list_of_movies.append(selected_url)
  number_of_movie += 1
  url_of_movie = selected_url
  get_request_of_movie = requests.get(url_of_movie, headers = site_header)
  souped_request_of_movie = BeautifulSoup(get_request_of_movie.content, 'lxml')
  content_of_movie = souped_request_of_movie.find('div', {'class': 'movie_synopsis clamp clamp-6 js-clamp'})

  #normal print -> here
  print(number_of_movie, selected_url)
  print('Movie Title : ' + each_movie.string.strip())
  print('Movie information : ' + content_of_movie.string.strip())

  #print in text file
  f = open("crawler_result.txt", "a")
  f.write(str(number_of_movie))
  f.write("  ")
  f.write(selected_url)
  f.write("\r\n")
  f.write('Movie Title : ' + each_movie.string.strip())
  f.write("\r\n")
  f.write('Movie information : ' + content_of_movie.string.strip())
  f.write("\r\n\r\n")
  f.close()

  #print in excel file
  crawler_table.write(selected_line , 0 , number_of_movie)
  crawler_table.write(selected_line , 1 , each_movie.string.strip())
  crawler_table.write(selected_line , 2 , selected_url)
  crawler_table.write(selected_line , 3 , content_of_movie.string.strip())
  #each time ++ line because we want to have seperate lines
  selected_line += 1

  enviroment.save('crawler_result.xls')